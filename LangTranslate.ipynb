{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOz5nmjRvcjTXRRpFN1vqap",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prince24-web/Google-colabs/blob/main/LangTranslate.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xRg9Gq3hY6YP",
        "outputId": "a0ce2b7d-1be4-4c88-dd2d-d9e67039a9c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello world\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5Mm6rpRstvII"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "userdata.get('GEMINI_API_KEY')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "-sYt0usRbyEK",
        "outputId": "16dc98f1-0aca-4465-aef6-fe3c00d7c01d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'AIzaSyA7t2CuIUqwpuGik5LN-m-UKXy58jxBwYA'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "\n",
        "# Get your secret\n",
        "api_key = userdata.get(\"GEMINI_API_KEY\")"
      ],
      "metadata": {
        "id": "KtkWFi8udup0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0289b20d"
      },
      "source": [
        "First, you need to import the `google.generativeai` library and configure it with your API key."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cf3ace36"
      },
      "source": [
        "import google.generativeai as genai\n",
        "from google.colab import userdata\n",
        "\n",
        "GOOGLE_API_KEY = userdata.get('GEMINI_API_KEY') # Make sure you have set this secret\n",
        "genai.configure(api_key=GOOGLE_API_KEY)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "276bccb7"
      },
      "source": [
        "Next, initialize the GenerativeModel. You can specify the model you want to use. In this example, we'll use `gemini-1.5-flash-latest`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "175a4b52"
      },
      "source": [
        "model = genai.GenerativeModel('gemini-2.0-flash')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b54c4c08"
      },
      "source": [
        "Finally, you can use the `generate_content` method to make an API call."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "87a690cb",
        "outputId": "563b52fd-5a16-427b-ebfd-3037bfe6590a"
      },
      "source": [
        "response = model.generate_content(\"Tell me a fun fact.\")\n",
        "print(response.text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here's a fun fact for you:\n",
            "\n",
            "**Octopuses have three hearts!** Two of them pump blood through the gills, and the third pumps blood to the rest of the body. This third heart actually stops beating when the octopus swims, which is why they prefer to crawl along the ocean floor.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from google.colab import userdata\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.0-flash\",   # âœ… use the correct name from list_models()\n",
        "    google_api_key=userdata.get(\"GEMINI_API_KEY\"),\n",
        "    temperature=0.3,\n",
        "    max_output_tokens=256,\n",
        "    top_k=40,\n",
        "    top_p=0.95,\n",
        ")\n",
        "\n",
        "result = llm.invoke(\"Explain LangChain to me like I'm 10 years old.\")\n",
        "print(result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zjRXfNWidJNm",
        "outputId": "91b61abc-2567-4c82-817e-b234de8964ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content=\"Okay, imagine you have a super smart robot friend who knows almost everything because it's read tons of books and websites. This robot is like a big language model, like the ones that power things like ChatGPT.\\n\\nBut, sometimes your robot friend needs a little help to answer your questions really well.  That's where LangChain comes in!\\n\\nLangChain is like a toolbox for your robot friend. It gives it special tools to:\\n\\n*   **Remember things:** Imagine you're talking to your robot friend about your favorite color. LangChain helps it remember that later in the conversation so it doesn't forget! It's like giving your robot friend a good memory.\\n\\n*   **Look things up:** If you ask your robot friend a question it doesn't know the answer to, LangChain helps it go online and search for the information. It's like giving your robot friend a super-powered search engine.\\n\\n*   **Use other tools:** Imagine you want your robot friend to draw a picture. LangChain can help it connect to a drawing program and tell it what to draw! It's like giving your robot friend the ability to use all sorts of different apps.\\n\\n*   **Think step-by-step\" additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'MAX_TOKENS', 'safety_ratings': []} id='run--a787838f-da94-45be-b246-a447feab3600-0' usage_metadata={'input_tokens': 15, 'output_tokens': 256, 'total_tokens': 271, 'input_token_details': {'cache_read': 0}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import init_chat_model\n",
        "\n",
        "model = init_chat_model(\"gemini-2.0-flash\", model_provider=\"google_genai\", google_api_key=userdata.get(\"GEMINI_API_KEY\"))"
      ],
      "metadata": {
        "id": "4n0hBahrvKq4"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=\"Translate the following from English into Italian\"),\n",
        "    HumanMessage(content=\"I like tomatoes, what should I eat?\")\n",
        "]\n",
        "\n",
        "model.invoke(messages)\n",
        "for token in model.stream(messages):\n",
        "    print(token.content, end=\"|\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tX49OjX6vzP1",
        "outputId": "f103986c-ee7a-43f5-9b22-6d84ebd2f400"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mi| piacciono i pomodori, cosa dovrei mangiare?|"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.invoke(\"Hello\")\n",
        "\n",
        "model.invoke([{\"role\": \"user\", \"content\": \"Hello\"}])\n",
        "\n",
        "model.invoke([HumanMessage(\"Hello\")])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6cf6_nzkwqcV",
        "outputId": "4630adae-bc45-4929-80d6-b09d8b951eed"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Hello! How can I help you today?', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run--439cc05d-2396-4458-8712-3a55fd4a3a38-0', usage_metadata={'input_tokens': 1, 'output_tokens': 10, 'total_tokens': 11, 'input_token_details': {'cache_read': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "system_template = \"Translate the following from English into {language}\"\n",
        "prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [(\"system\", system_template), (\"user\", \"{text}\")]\n",
        ")"
      ],
      "metadata": {
        "id": "gYtl8lS8x8ZS"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = prompt_template.invoke({\"language\": \"Italian\", \"text\": \"hi!\"})\n",
        "\n",
        "prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UB07oMmlybKa",
        "outputId": "e0661f36-340a-423b-c825-9420ebaa55c0"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptValue(messages=[SystemMessage(content='Translate the following from English into Italian', additional_kwargs={}, response_metadata={}), HumanMessage(content='hi!', additional_kwargs={}, response_metadata={})])"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "an4A7pL5y0cK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt.to_messages()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ssadn6piy07U",
        "outputId": "0464a9dd-250d-425c-ad29-1c7e88fdc037"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[SystemMessage(content='Translate the following from English into Italian', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='hi!', additional_kwargs={}, response_metadata={})]"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.invoke(prompt)\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c28_EspA0Fhp",
        "outputId": "5af2e4cf-171e-482a-bf67-a658fba8d8a5"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ciao!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts  import ChatPromptTemplate\n",
        "\n",
        "system_template = \"Translate the following from English to {language}\"\n",
        "prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [(\"system\",system_template),(\"user\",\"{text}\")]\n",
        ")\n",
        "\n",
        "prompt = prompt_template.invoke({\"language\":\"dutch\", \"text\":\"you eat a sandwish\"})\n",
        "prompt.to_messages()\n",
        "response = model.invoke(prompt)\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mm61VQx-0QPv",
        "outputId": "51803914-df64-4792-a52d-4c0f759be429"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Je eet een boterham.\n"
          ]
        }
      ]
    }
  ]
}